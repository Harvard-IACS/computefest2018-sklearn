{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.13", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }, 
  "nbformat": 4, 
  "nbformat_minor": 0, 
  "cells": [
    {
      "source": [
        "# Learning a model"
      ], 
      "cell_type": "markdown", 
      "metadata": {
        "hide": true
      }
    }, 
    {
      "execution_count": 42, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline\n", 
        "import numpy as np\n", 
        "import scipy as sp\n", 
        "import matplotlib as mpl\n", 
        "import matplotlib.cm as cm\n", 
        "import matplotlib.pyplot as plt\n", 
        "import pandas as pd\n", 
        "pd.set_option('display.width', 500)\n", 
        "pd.set_option('display.max_columns', 100)\n", 
        "pd.set_option('display.notebook_repr_html', True)\n", 
        "import seaborn as sns\n", 
        "sns.set_style(\"whitegrid\")\n", 
        "sns.set_context(\"poster\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "hide": true
      }
    }, 
    {
      "execution_count": 43, 
      "cell_type": "code", 
      "source": [
        "def make_simple_plot():\n", 
        "    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n", 
        "    axes[0].set_ylabel(\"$y$\")\n", 
        "    axes[0].set_xlabel(\"$x$\")\n", 
        "    axes[1].set_xlabel(\"$x$\")\n", 
        "    axes[1].set_yticklabels([])\n", 
        "    axes[0].set_ylim([-2,2])\n", 
        "    axes[1].set_ylim([-2,2])\n", 
        "    plt.tight_layout();\n", 
        "    return axes\n", 
        "def make_plot():\n", 
        "    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n", 
        "    axes[0].set_ylabel(\"$p_R$\")\n", 
        "    axes[0].set_xlabel(\"$x$\")\n", 
        "    axes[1].set_xlabel(\"$x$\")\n", 
        "    axes[1].set_yticklabels([])\n", 
        "    axes[0].set_ylim([0,1])\n", 
        "    axes[1].set_ylim([0,1])\n", 
        "    axes[0].set_xlim([0,1])\n", 
        "    axes[1].set_xlim([0,1])\n", 
        "    plt.tight_layout();\n", 
        "    return axes"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "hide": true
      }
    }, 
    {
      "execution_count": 44, 
      "cell_type": "code", 
      "source": [
        "df=pd.read_csv(\"data/religion.csv\")\n", 
        "df.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 45, 
      "cell_type": "code", 
      "source": [
        "x=df.rfrac.values\n", 
        "f=df.promney.values"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 46, 
      "cell_type": "code", 
      "source": [
        "#allindexes=np.sort(np.random.choice(x.shape[0], size=100, replace=False))\n", 
        "indexes=np.sort(np.random.choice(x.shape[0], size=30, replace=False))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 47, 
      "cell_type": "code", 
      "source": [
        "samplex = x[indexes]\n", 
        "samplef = f[indexes]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 48, 
      "cell_type": "code", 
      "source": [
        "sigma=0.06\n", 
        "mask=(x > 0.65) & (x < 0.8)\n", 
        "sigmalist=sigma+mask*0.03"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 49, 
      "cell_type": "code", 
      "source": [
        "y = f + sp.stats.norm.rvs(scale=sigmalist, size=200)\n", 
        "#the next three lines just ensure that y remains a probability\n", 
        "yadd = (y < 0.0) *(0.01-y)\n", 
        "ysub = (y > 1.0)*(y - 1.0)\n", 
        "y = y + yadd -ysub"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 50, 
      "cell_type": "code", 
      "source": [
        "plt.plot(x,f, 'r-', alpha=0.6, label=\"f\");\n", 
        "plt.plot(x[indexes], y[indexes], 's', alpha=0.6, label=\"in-sample y (observed)\");\n", 
        "plt.plot(x, y, '.', alpha=0.6, label=\"population y\");\n", 
        "plt.xlabel('$x$');\n", 
        "plt.ylabel('$p_R$')\n", 
        "plt.legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "execution_count": 51, 
      "cell_type": "code", 
      "source": [
        "df=pd.DataFrame(dict(x=x[indexes],f=f[indexes],y=y[indexes]))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 52, 
      "cell_type": "code", 
      "source": [
        "from sklearn.cross_validation import train_test_split\n", 
        "datasize=df.shape[0]\n", 
        "#split dataset using the index, as we have x,f, and y that we want to split.\n", 
        "itrain,itest = train_test_split(range(30),train_size=24, test_size=6)\n", 
        "xtrain= df.x[itrain].values\n", 
        "ftrain = df.f[itrain].values\n", 
        "ytrain = df.y[itrain].values\n", 
        "xtest= df.x[itest].values\n", 
        "ftest = df.f[itest].values\n", 
        "ytest = df.y[itest].values"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 53, 
      "cell_type": "code", 
      "source": [
        "def make_features(train_set, test_set, degrees):\n", 
        "    traintestlist=[]\n", 
        "    for d in degrees:\n", 
        "        traintestdict={}\n", 
        "        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n", 
        "        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n", 
        "        traintestlist.append(traintestdict)\n", 
        "    return traintestlist"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 54, 
      "cell_type": "code", 
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n", 
        "from sklearn.linear_model import LinearRegression\n", 
        "from sklearn.metrics import mean_squared_error\n", 
        "\n", 
        "degrees=range(21)\n", 
        "error_train=np.empty(len(degrees))\n", 
        "error_test=np.empty(len(degrees))\n", 
        "\n", 
        "traintestlists=make_features(xtrain, xtest, degrees)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 55, 
      "cell_type": "code", 
      "source": [
        "for d in degrees:#for increasing polynomial degrees 0,1,2...\n", 
        "    Xtrain = traintestlists[d]['train']\n", 
        "    Xtest = traintestlists[d]['test']\n", 
        "    #set up model\n", 
        "    est = LinearRegression()\n", 
        "    #fit\n", 
        "    est.fit(Xtrain, ytrain)\n", 
        "    #predict\n", 
        "    prediction_on_training = est.predict(Xtrain)\n", 
        "    prediction_on_test = est.predict(Xtest)\n", 
        "    #calculate mean squared error\n", 
        "    error_train[d] = mean_squared_error(ytrain, prediction_on_training)\n", 
        "    error_test[d] = mean_squared_error(ytest, prediction_on_test)\n", 
        "\n", 
        "plt.plot(degrees, error_train, marker='o', label='train (in-sample)')\n", 
        "plt.plot(degrees, error_test, marker='o', label='test')\n", 
        "plt.axvline(np.argmin(error_test), 0,0.5, color='r', label=\"min test error at d=%d\"%np.argmin(error_test), alpha=0.3)\n", 
        "plt.ylabel('mean squared error')\n", 
        "plt.xlabel('degree')\n", 
        "plt.legend(loc='upper left')\n", 
        "plt.yscale(\"log\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "![m:caption](images/complexity-error-plot.png)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Validation"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "There is a problem with the process above which is not apparent at first look. What we have done in picking a given $d$ as the best hypothesis is that we have used the test set as a training set. How?\n", 
        "\n", 
        "Our process used the training set to fit for the **parameters**(values of the coefficients) of the polynomial of given degree $d$ based on minimizing the traing set error (empirical risk minimization). We then calculated the error on the test set at that $d$. If we go further and choose the best $d$ based on minimizing the test set error, we have then \"fit for\" $d$ on the test set. We will thus call $d$ a **hyperparameter** of the model.\n", 
        "\n", 
        "In this case, the test-set error will underestimate the true out-of-sample error (for a proof of this see Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data. AMLBook, 2012.). Furthermore, we have **contaminated the test set** by fitting for $d$ on it; it is no longer a true test set.\n", 
        "\n", 
        "Thus, we must introduce a new **validation set** on which the complexity parameter $d$ is fit, and leave out a test set which we can use to estimate the true out-of-sample performance of our learner. The place of this set in the scheme of things is shown below:\n", 
        "\n", 
        "![m:caption](images/train-validate-test.png)\n", 
        "\n", 
        "We have split the old training set into a training set and a validation set, holding the old test aside for FINAL testing AFTER we have \"fit\" for complexity $d$. Obviously we have decreased the size of the data available for training further, but this is a price we must pay for obtaining a good estimate of the out-of-sample risk $\\cal{E}_{out}$ (also denoted as risk $R_{out}$) through the test risk $\\cal{E}_{test}$ ($R_{test}$).\n", 
        "\n", 
        "![m:caption](images/train-validate-test-cont.png)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The validation process is illustrated in these two figures. We first loop ober all the hypothesis sets that we wish to consider: in our case this is a loop over the complexity parameter $d$, the degree of the polynomials we will try and fit. Then for each degree $d$, we obtain a best fit model $g^-_d$ where the \"minus\" superscript indicates that we fit our model on the new training set which is obtained by removing (\"minusing\") a validation chunk (often the same size as the test chunk) from the old training set. We then \"test\" this model on the validation chunk, obtaining the validation error for the best-fit polynomial coefficients and for degree $d$. We move on to the next degree $d$ and repeat the process, just like before. We compare all the validation set errors, just like we did with the test errors earlier, and pick the degree $d_*$ which minimizes this validation set error.\n", 
        "\n", 
        "![caption](images/train-validate-test3.png)\n", 
        "\n", 
        "Having picked the hyperparameter $d_*$, we retrain using the hypothesis set $\\cal{H}_{*}$ on the entire old training-set to find the parameters of the polynomial of order $d_*$ and the corresponding best fit hypothesis $g_*$. Note that we left the minus off the $g$ to indicate that it was trained on the entire old traing set. We now compute the test error on the test set as an estimate of the test risk $\\cal{E}_{test}$.\n", 
        "\n", 
        "Thus the **validation** set if the set on which the hyperparameter is fit. This method of splitting the data $\\cal{D}$ is called the **train-validate-test** split.\n", 
        "\n", 
        "We carry out this process for one training/validation split below. Note the smaller size of the new training set. We hold the test set at the same size.\n", 
        "\n", 
        "YOUR TURN HERE:\n", 
        ">fit on the whole training set below after finding the best model. Store in variable `err` the mean squared error."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 56, 
      "cell_type": "code", 
      "source": [
        "#we split the training set down further\n", 
        "intrain,invalid = train_test_split(itrain,train_size=18, test_size=6)\n", 
        "xntrain= df.x[intrain].values\n", 
        "fntrain = df.f[intrain].values\n", 
        "yntrain = df.y[intrain].values\n", 
        "xnvalid= df.x[invalid].values\n", 
        "fnvalid = df.f[invalid].values\n", 
        "ynvalid = df.y[invalid].values\n", 
        "\n", 
        "degrees=range(21)\n", 
        "error_train=np.empty(len(degrees))\n", 
        "error_valid=np.empty(len(degrees))\n", 
        "trainvalidlists=make_features(xntrain, xnvalid, degrees)\n", 
        "\n", 
        "#we now train on the smaller training set\n", 
        "for d in degrees:#for increasing polynomial degrees 0,1,2...\n", 
        "    #Create polynomials from x\n", 
        "    Xntrain = trainvalidlists[d]['train']\n", 
        "    Xnvalid = trainvalidlists[d]['test']\n", 
        "    #fit a model linear in polynomial coefficients on the new smaller training set\n", 
        "    est = LinearRegression()\n", 
        "    est.fit(Xntrain, yntrain)\n", 
        "    #predict on new training and validation sets and calculate mean squared error\n", 
        "    error_train[d] = mean_squared_error(yntrain, est.predict(Xntrain))\n", 
        "    error_valid[d] = mean_squared_error(ynvalid, est.predict(Xnvalid))\n", 
        "\n", 
        "#calculate the degree at which validation error is minimized\n", 
        "mindeg = np.argmin(error_valid)\n", 
        "#need to remake polynomial features on the whole training set\n", 
        "ttlist=make_features(xtrain, xtest, degrees)\n", 
        "features_at_mindeg = ttlist[mindeg]['train']\n", 
        "test_features_at_mindeg = ttlist[mindeg]['test']\n", 
        "#fit on whole training set now. Put MSE in variable err.\n", 
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 57, 
      "cell_type": "code", 
      "source": [
        "plt.plot(degrees, error_train, marker='o', label='train (in-sample)')\n", 
        "plt.plot(degrees, error_valid, marker='o', label='validation')\n", 
        "plt.plot([mindeg], [err], marker='s', markersize=10, label='test', alpha=0.5, color='r')\n", 
        "plt.ylabel('mean squared error')\n", 
        "plt.xlabel('degree')\n", 
        "plt.legend(loc='upper left')\n", 
        "plt.yscale(\"log\")\n", 
        "print(mindeg)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets do this again, choosing a new random split between training and validation data: "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 58, 
      "cell_type": "code", 
      "source": [
        "intrain,invalid = train_test_split(itrain,train_size=18, test_size=6)\n", 
        "xntrain= df.x[intrain].values\n", 
        "fntrain = df.f[intrain].values\n", 
        "yntrain = df.y[intrain].values\n", 
        "xnvalid= df.x[invalid].values\n", 
        "fnvalid = df.f[invalid].values\n", 
        "ynvalid = df.y[invalid].values\n", 
        "\n", 
        "degrees=range(21)\n", 
        "error_train=np.empty(len(degrees))\n", 
        "error_valid=np.empty(len(degrees))\n", 
        "trainvalidlists=make_features(xntrain, xnvalid, degrees)\n", 
        "\n", 
        "for d in degrees:#for increasing polynomial degrees 0,1,2...\n", 
        "    #Create polynomials from x\n", 
        "    Xntrain = trainvalidlists[d]['train']\n", 
        "    Xnvalid = trainvalidlists[d]['test']\n", 
        "    #fit a model linear in polynomial coefficients on the training set\n", 
        "    est = LinearRegression()\n", 
        "    est.fit(Xntrain, yntrain)\n", 
        "    #calculate mean squared error\n", 
        "    error_train[d] = mean_squared_error(yntrain, est.predict(Xntrain))\n", 
        "    error_valid[d] = mean_squared_error(ynvalid, est.predict(Xnvalid))\n", 
        "\n", 
        "mindeg = np.argmin(error_valid)\n", 
        "ttlist=make_features(xtrain, xtest, degrees)\n", 
        "features_at_mindeg = ttlist[mindeg]['train']\n", 
        "test_features_at_mindeg = ttlist[mindeg]['test']\n", 
        "clf = LinearRegression()\n", 
        "clf.fit(features_at_mindeg, ytrain) # fit\n", 
        "pred = clf.predict(test_features_at_mindeg)\n", 
        "err = mean_squared_error(ytest, pred)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 59, 
      "cell_type": "code", 
      "source": [
        "plt.plot(degrees, error_train, marker='o', label='train (in-sample)')\n", 
        "plt.plot(degrees, error_valid, marker='o', label='validation')\n", 
        "plt.plot([mindeg], [err], marker='s', markersize=10, label='test', alpha=0.5, color='r')\n", 
        "\n", 
        "plt.ylabel('mean squared error')\n", 
        "plt.xlabel('degree')\n", 
        "plt.legend(loc='lower left')\n", 
        "plt.yscale(\"log\")\n", 
        "print(mindeg)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "This time the validation error minimizing polynomial degree might change! What happened?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Cross Validation"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The problem"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "1. Since we are dealing with small data sizes here, you should worry that a given split exposes us to the peciliarity of the data set that got randomly chosen for us. This naturally leads us to want to choose multiple such random splits and somehow average over this process to find the \"best\" validation minimizing polynomial degree or complexity $d$.\n", 
        "2. The multiple splits process also allows us to get an estimate of how consistent our prediction error is: in other words, just like in the hair example, it gives us a distribution.\n", 
        "3. Furthermore the validation set that we left out has two competing demands on it. The larger the set is, the better is our estimate of the out-of-sample error. So we'd like to hold out as much as possible. But the smaller the validation set is, the more data we have to train ourmodel on. Thus we can fit a better, more expressive model. We want to balance these two desires, and additionally, not be exposed to any peculiarities that might randomly arise in any single train-validate split of the old training set."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The Idea"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "To deal with this we engage in a process called **cross-validation**, which is illustrated in the figure below, for a given hypothesis set $\\cal{H}_a$ with complexity parameter $d=a$ (the polynomial degree). We do the train/validate split, not once but multiple times. \n", 
        "\n", 
        "In the figure below we create 4-folds from the training set part of our data set $\\cal{D}$. By this we mean that we divide our set roughly into 4 equal parts. As illustrated below, this can be done in 4 different ways, or folds. In each fold we train a model on 3 of the parts. The model so trained is denotet as $g^-_{Fi}$, for example $g^-_{F3}$ . The minus sign in the superscript once again indicates that we are training on a reduced set. The $F3$ indicates that this model was trained on the third fold. Note that the model trained on each fold will be different!\n", 
        "\n", 
        "For each fold, after training the model, we calculate the risk or error on the remaining one validation part. We then add the validation errors together from the different folds, and divide by the number of folds to calculate an average error. Note again that this average error is an average over different models $g^-_{Fi}$. We use this error as the validation error for $d=a$ in the validation process described earlier."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "![m:caption](images/train-cv2.png)\n", 
        "\n", 
        "Note that the number of folds is equal to the number of splits in the data. For example, if we have 5 splits, there will be 5 folds. To illustrate cross-validation consider below fits in $\\cal{H}_0$ and $\\cal{H}_1$ (means and straight lines) to a sine curve, with only 3 data points."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The entire description of K-fold Cross-validation"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We put thogether this scheme to calculate the error for a given polynomial degree $d$ with the method we used earlier to choose a model given the validation-set risk as a function of $d$:\n", 
        "\n", 
        "1. create `n_folds` partitions of the training data. \n", 
        "2. We then train on `n_folds -1` of these partitions, and test on the remaining partition. There are `n_folds` such combinations of partitions (or folds), and thus we obtain `n_fold` risks.\n", 
        "3. We average the error or risk of all such combinations to obtain, for each value of $d$, $R_{dCV}$.\n", 
        "4. We move on to the next value of $d$, and repeat 3\n", 
        "5. and then find the optimal value of d that minimizes risk $d=*$.\n", 
        "5. We finally use that value to make the final fit in $\\cal{H}_*$ on the entire old training set.\n", 
        "\n", 
        "![caption](images/train-cv3.png)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "It can also shown that **cross-validation error is an unbiased estimate of the out of sample-error**.\n", 
        "\n", 
        "Let us now do 4-fold cross-validation on our Romney votes data set. We increase the complexity from degree 0 to degree 20. In each case we take the old training set, split in 4 ways into 4 folds, train on 3 folds, and calculate the validation error on the ramining one. We then average the erros over the four folds to get a cross-validation error for that $d$. Then we did what we did before: find the hypothesis space $\\cal{H}_*$ with the lowest cross-validation error, and refit it using the entire training set. We can then use the test set to estimate $E_{out}$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 60, 
      "cell_type": "code", 
      "source": [
        "from sklearn.cross_validation import KFold\n", 
        "n_folds=4\n", 
        "degrees=range(21)\n", 
        "results=[]\n", 
        "for d in degrees:\n", 
        "    hypothesisresults=[]\n", 
        "    for train, test in KFold(24, n_folds): # split data into train/test groups, 4 times\n", 
        "        tvlist=make_features(xtrain[train], xtrain[test], degrees)\n", 
        "        clf = LinearRegression()\n", 
        "        clf.fit(tvlist[d]['train'], ytrain[train]) # fit\n", 
        "        hypothesisresults.append(mean_squared_error(ytrain[test], clf.predict(tvlist[d]['test']))) # evaluate score function on held-out data\n", 
        "    results.append((np.mean(hypothesisresults), np.min(hypothesisresults), np.max(hypothesisresults), np.std(hypothesisresults))) # average"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 66, 
      "cell_type": "code", 
      "source": [
        "mindeg = np.argmin([r[0] for r in results])\n", 
        "ttlist=make_features(xtrain, xtest, degrees)\n", 
        "#fit on whole training set now.\n", 
        "clf = LinearRegression()\n", 
        "clf.fit(ttlist[mindeg]['train'], ytrain) # fit\n", 
        "pred = clf.predict(ttlist[mindeg]['test'])\n", 
        "err = mean_squared_error(pred, ytest)\n", 
        "errtr=mean_squared_error(ytrain, clf.predict(ttlist[mindeg]['train']))\n", 
        "errout=0.8*errtr+0.2*err\n", 
        "c0=sns.color_palette()[0]\n", 
        "c1=sns.color_palette()[1]\n", 
        "#plt.errorbar(degrees, [r[0] for r in results], yerr=[r[1] for r in results], marker='o', label='CV error', alpha=0.5)\n", 
        "plt.plot(degrees, [r[0] for r in results], marker='o', label='CV error', alpha=0.9)\n", 
        "plt.fill_between(degrees, [r[1] for r in results], [r[2] for r in results], color=c0, alpha=0.2)\n", 
        "\n", 
        "\n", 
        "plt.plot([mindeg], [err], 'o',  label='test set error')\n", 
        "plt.plot([mindeg], [errout], 'o',  label='full sample error')\n", 
        "\n", 
        "\n", 
        "plt.ylabel('mean squared error')\n", 
        "plt.xlabel('degree')\n", 
        "plt.legend(loc='upper right')\n", 
        "plt.yscale(\"log\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We see that the cross-validation error minimizes at a low degree, and then increases. Because we have so few data points the spread in fold errors increases as well."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Regularization"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Upto now we have focussed on finding the polynomial with the right degree of complecity $d=*$ given the data that we have.\n", 
        "\n", 
        "Let us now ask a different question: if we are going to fit the data with an expressive model such as 20th order polynomials, how can we **regularize** or smooth or restrict the choices of the kinds of 20th order polynomials that we allow in our fits. In other words, we are again trying to bring down the complexity of the hypothesis space, but by a different tack: a tack which prefers smooth polynomials over wiggly ones.\n", 
        "\n", 
        "That is, if we want to fit with a 20th order polynomial, ok, lets fit with it, but lets reduce the size of, or limit the functions in $\\cal{H}_{20}$ that we allow.\n", 
        "\n", 
        "In a sense we have already done this, havent we? When we compared $\\cal{H}_1$ over $\\cal{H}_{20}$, we imposed a **hard constraint** by setting all polynomial co-efficients for terms higher than $x$ to 0. In other words you can think of a line as a 20th degree polynomial with many 0 coefficients. Why not have the math machinery do this for you than do it by hand, and use the data to figure out how to make the cut. \n", 
        "\n", 
        "We do this by a **soft constraint** by setting:\n", 
        "\n", 
        "$$\\sum_{i=0}^j a_i^2 < C.$$\n", 
        "\n", 
        "This setting is called the Ridge.\n", 
        "\n", 
        "This ensures that the coefficients dont get too high, which makes sure we dont get wildly behaving pilynomials with high coefficients. If we set \n", 
        "\n", 
        "$$\\sum_{i=0}^j | a_i | < C,$$\n", 
        "\n", 
        "it can be shown that some coefficients will be set to exactly 0. This is called the Lasso.\n", 
        "\n", 
        "It turns out that we can do this by adding a term to the empirical risk that we minimize on the training data for $\\cal{H}_j$ (seeing why is beyond the scope here but google on lagrange multipliers and the dual problem):\n", 
        "\n", 
        "$$\\cal{R}(h_j) =  \\sum_{y_i \\in \\cal{D}} (y_i - h_j(x_i))^2 +\\alpha \\sum_{i=0}^j a_i^2.$$\n", 
        "\n", 
        "This new risk takes the empirical risk and adds a \"penalty term\" to it to minimize overfitting. The term is proportional to the sum of the squares of the coefficients and is positive, so it will keep their values down\n", 
        "\n", 
        "Notice that we are adding a term to the **training error**, once $\\alpha$ is defined. The entire structure is similar to what we did to find the optimal $d=*$, with $\\alpha$ being the analog of $d$. And thus we can use the same validation and cross-validation technology that we developed to estimate $d$.\n", 
        "\n", 
        "This technique is called **regularization** or **shrinkage** as it takes the coefficients $a_i$ towards smaller sizes. As you have seen earlier, for polynomials this corresponds to choosing smooth functions over wiggly functions. When $\\alpha=0$ we have the regular polynomial regression problem, and if we are using 20th order polynomials we will wildly overfit. We are in the high variance zone. The problem with a non-zero $\\alpha$ is called **ridge regression**. As $\\alpha$ increases, the importance of the penalty term increases at the expense of the ERM term, and we are pushed to increase the smoothness of the polynomials. When $\\alpha$ becomes very large the penalty term dominates and we get into the high bias zone. Thus $\\alpha$ acts as a complexity parameter just like $d$ did, with high complexity being $\\alpha \\to 0$.\n", 
        "\n", 
        "Even though we are not doing any proofs, let us illustrate the concept of regularization using a line fit to a sine wave. We fit a straight line to 3 points, choosing 100 such sets of 3 points from the data set."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 23, 
      "cell_type": "code", 
      "source": [
        "xs=np.arange(-1.,1.,0.01)\n", 
        "ff = lambda x: np.sin(np.pi*x)\n", 
        "ffxs=ff(xs)\n", 
        "axes=make_simple_plot()\n", 
        "c0=sns.color_palette()[0]\n", 
        "c1=sns.color_palette()[1]\n", 
        "axes[0].plot(xs, ff(xs), alpha=0.9, lw=3, color=c0)\n", 
        "axes[1].plot(xs, ff(xs), alpha=0.9, lw=3, color=c0)\n", 
        "from sklearn.linear_model import Ridge\n", 
        "D=np.empty((100,3), dtype=\"int\")\n", 
        "print(D.shape)\n", 
        "for i in range(100):\n", 
        "    D[i,:] = np.random.choice(200, replace=False, size=3)\n", 
        "for i in range(100):\n", 
        "    choices = D[i,:]\n", 
        "    #regular fit\n", 
        "    p1=np.polyfit(xs[choices], ffxs[choices],1)\n", 
        "    #ridge fit\n", 
        "    est = Ridge(alpha=1.0)\n", 
        "    est.fit(xs[choices].reshape(-1,1), ffxs[choices])\n", 
        "    axes[0].plot(xs, np.polyval(p1, xs), color=c1, alpha=0.2)\n", 
        "    axes[1].plot(xs, est.predict(xs.reshape(-1,1)), color=c1, alpha=0.2)\n", 
        "axes[0].set_title(\"Unregularized\");\n", 
        "axes[1].set_title(\"Regularized with $\\\\alpha=1.0$\");"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "In the left panel we plot unregularized straight line fits. The plot is hairy, since choosing 3 points from 200 between -1 and 1 dosent constrain the lines much at all. On the right panel, we plot the output of **Ridge** regression with $\\alpha=1$. This corresponds to adding a term to the empirical risk of $\\alpha\\, (a_0^2 + a_1^2)$ where $a_0$ and $a_1$ are the intercept and slope of the line respectively. Notice that the lines are much more constrained  in this second plot. The penalty term has regularized the values of the intercept and slope, and forced the intercept to be closer to 0 and the lines to be flatter."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Regularization of the Romney model with Cross-Validation"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 24, 
      "cell_type": "code", 
      "source": [
        "def plot_functions(est, ax, df, alpha, xtest, Xtest, xtrain, ytrain):\n", 
        "    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n", 
        "    ax.plot(df.x, df.f, color='k', label='f')\n", 
        "    ax.plot(xtrain, ytrain, 's', label=\"training\", alpha=0.4)\n", 
        "    ax.plot(xtest, ytest, 's', label=\"testing\", alpha=0.6)\n", 
        "    transx=np.arange(0,1.1,0.01)\n", 
        "    transX = PolynomialFeatures(20).fit_transform(transx.reshape(-1,1))\n", 
        "    ax.plot(transx, est.predict(transX),  '.', label=\"alpha = %s\" % str(alpha))\n", 
        "    ax.set_ylim((0, 1))\n", 
        "    ax.set_xlim((0, 1))\n", 
        "    ax.set_ylabel('y')\n", 
        "    ax.set_xlabel('x')\n", 
        "    ax.legend(loc='lower right')\n", 
        "    \n", 
        "def plot_coefficients(est, ax, alpha):\n", 
        "    coef = est.coef_.ravel()\n", 
        "    ax.semilogy(np.abs(coef), marker='o', label=\"alpha = %s\" % str(alpha))\n", 
        "    ax.set_ylim((1e-1, 1e15))\n", 
        "    ax.set_ylabel('abs(coefficient)')\n", 
        "    ax.set_xlabel('coefficients')\n", 
        "    ax.legend(loc='upper left')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Lets now go back to the Romney voting model and see what regularization does to the fits in that model. The addition of a penalty term to the risk or error causes us to choose a smaller subset of the entire set of complex $\\cal{H}_{20}$ polynomials. This is shown in the diagram below where the balance between bias and variance occurs at some subset $S_*$ of the set of 20th order polynomials indexed by $\\alpha_*$ (there is an error on the diagram, the 13 there should actually be a 20).\n", 
        "\n", 
        "![m:caption](images/complexity-error-reg.png)\n", 
        "\n", 
        "Lets see what some of the $\\alpha$s do. The diagram below trains on the entire training set, for given values of $\\alpha$, minimizing the penalty-term-added training error."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 25, 
      "cell_type": "code", 
      "source": [
        "fig, rows = plt.subplots(4, 2, figsize=(12, 16))\n", 
        "d=20\n", 
        "alphas = [0.0, 1e-5, 1e-3, 1]\n", 
        "Xtrain = traintestlists[d]['train']\n", 
        "Xtest = traintestlists[d]['test']\n", 
        "for i, alpha in enumerate(alphas):\n", 
        "    l,r=rows[i]\n", 
        "    est = Ridge(alpha=alpha)\n", 
        "    est.fit(Xtrain, ytrain)\n", 
        "    plot_functions(est, l, df, alpha, xtest, Xtest, xtrain, ytrain )\n", 
        "    plot_coefficients(est, r, alpha)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "As you can see, as we increase $\\alpha$ from 0 to 1, we start out overfitting, then doing well, and then, our fits, develop a mind of their own irrespective of data, as the penalty term dominates the risk.\n", 
        "\n", 
        "Lets use cross-validation to figure what this critical $\\alpha_*$ is. To do this we use the concept of a *meta-estimator* from scikit-learn. As the API paper puts it:\n", 
        "\n", 
        ">In scikit-learn, model selection is supported in two distinct meta-estimators, GridSearchCV and RandomizedSearchCV. They take as input an estimator (basic or composite), whose hyper-parameters must be optimized, and a set of hyperparameter settings to search through.\n", 
        "\n", 
        "The concept of a meta-estimator allows us to wrap, for example, cross-validation, or methods that build and combine simpler models or schemes. For example:\n", 
        "\n", 
        "    clf = Ridge()\n", 
        "    parameters = {\"alpha\": [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\n", 
        "    gridclassifier=GridSearchCV(clf, param_grid=parameters, cv=4, scoring=\"mean_squared_error\")\n", 
        "    \n", 
        "The `GridSearchCV` replaces the manual iteration over thefolds using `KFolds` and the averaging we did previously, doint it all for us. It takes a parameter grid in the shape of a dictionary as input, and sets $\\alpha$ to the appropriate parameter values one by one. It then trains the model, cross-validation fashion, and gets the error. Finally it compares the errors for the different $\\alpha$'s, and picks the best choice model."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 26, 
      "cell_type": "code", 
      "source": [
        "from sklearn.metrics import make_scorer\n", 
        "#, 1e-6, 1e-5, 1e-3, 1.0\n", 
        "from sklearn.grid_search import GridSearchCV\n", 
        "def cv_optimize_ridge(X, y, n_folds=4):\n", 
        "    clf = Ridge()\n", 
        "    parameters = {\"alpha\": [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\n", 
        "    #the scoring parameter below is the default one in ridge, but you can use a different one\n", 
        "    #in the cross-validation phase if you want.\n", 
        "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, scoring=\"mean_squared_error\")\n", 
        "    gs.fit(X, y)\n", 
        "    return gs"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 27, 
      "cell_type": "code", 
      "source": [
        "fitmodel = cv_optimize_ridge(Xtrain, ytrain, n_folds=4)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 28, 
      "cell_type": "code", 
      "source": [
        "fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Our best model occurs for $\\alpha=0.01$. We also output the mean cross-validation error at different $\\alpha$ (with a negative sign, as scikit-learn likes to maximize negative error which is equivalent to minimizing error).\n", 
        "\n", 
        "We refit the estimator on old training set, and calculate and plot the test set error and the polynomial coefficients. Notice how many of these coefficients have been pushed to lower values or 0.\n", 
        "\n", 
        "YOUR TURN NOW:\n", 
        ">assign to variable clf the classifier obtained by fitting the entire training set using the best $\\alpha$ found above."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 29, 
      "cell_type": "code", 
      "source": [
        "#Store in clf a new classifier fit on the entire training set with the best alpha\n", 
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 30, 
      "cell_type": "code", 
      "source": [
        "def plot_functions_onall(est, ax, df, alpha, xtrain, ytrain, Xtrain, xtest, ytest):\n", 
        "    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n", 
        "    ax.plot(df.x, df.f, color='k', label='f')\n", 
        "    ax.plot(xtrain, ytrain, 's', alpha=0.4, label=\"train\")\n", 
        "    ax.plot(xtest, ytest, 's', alpha=0.6, label=\"test\")\n", 
        "    transx=np.arange(0,1.1,0.01)\n", 
        "    transX = PolynomialFeatures(20).fit_transform(transx.reshape(-1,1))\n", 
        "    ax.plot(transx, est.predict(transX), '.', alpha=0.6, label=\"alpha = %s\" % str(alpha))\n", 
        "    #print est.predict(transX)\n", 
        "    ax.set_ylim((0, 1))\n", 
        "    ax.set_xlim((0, 1))\n", 
        "    ax.set_ylabel('y')\n", 
        "    ax.set_xlabel('x')\n", 
        "    ax.legend(loc='lower right')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 31, 
      "cell_type": "code", 
      "source": [
        "fig, rows = plt.subplots(1, 2, figsize=(12, 5))\n", 
        "l,r=rows\n", 
        "plot_functions_onall(clf, l, df, alphawechoose, xtrain, ytrain, Xtrain, xtest, ytest)\n", 
        "plot_coefficients(clf, r, alphawechoose)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "As we can see, the best fit model is now chosen from the entire set of 20th order polynomials, and a non-zero hyperparameter $\\alpha$ that we fit for ensures that only smooth models amonst these polynomials are chosen, by setting most of the polynomial coefficients to something close to 0 (Lasson sets them exactly to 0)."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### What is minimized where?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Regularization adds a term to the error or risk, which is then, for fixed $\\alpha$, minimized on the training set.\n", 
        "The validation error is then calculated on the validation set and reported alongside the value of $\\alpha$ used. If we cross-validate, then the validation error is an average over multiple folds.\n", 
        "\n", 
        "On the other hand, when we validate for $d$, we use the validation error as an estimate of the out-of-sample error and use this estimate to directly pick the best model (best $d$).\n", 
        "\n", 
        "One can think of **regularization as an estimation of the overfit term** while **validation directly estimates $R_{out}$**."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## A multiple regression problem: Boston Dataset"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 32, 
      "cell_type": "code", 
      "source": [
        "from sklearn.datasets import load_boston\n", 
        "boston = load_boston()\n", 
        "print(boston.keys())"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 33, 
      "cell_type": "code", 
      "source": [
        "print(boston.DESCR)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 34, 
      "cell_type": "code", 
      "source": [
        "plt.hist(boston.target, bins=30);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 67, 
      "cell_type": "code", 
      "source": [
        "boston.data"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 68, 
      "cell_type": "code", 
      "source": [
        "boston.feature_names"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 76, 
      "cell_type": "code", 
      "source": [
        "boston.data[:,-1]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 77, 
      "cell_type": "code", 
      "source": [
        "dfboston=pd.DataFrame({k:boston.data[:,i] for i,k in enumerate(boston.feature_names)})\n", 
        "dfboston.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 105, 
      "cell_type": "code", 
      "source": [
        "itrain, itest = train_test_split(range(len(boston.target)))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 107, 
      "cell_type": "code", 
      "source": [
        "dfboston.iloc[itrain]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 108, 
      "cell_type": "code", 
      "source": [
        "boston.target[itrain]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 109, 
      "cell_type": "code", 
      "source": [
        "lr = LinearRegression()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 110, 
      "cell_type": "code", 
      "source": [
        "lr.fit(dfboston[['LSTAT','AGE']].iloc[itrain], boston.target[itrain])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 111, 
      "cell_type": "code", 
      "source": [
        "mean_squared_error(boston.target[itest], lr.predict(dfboston[['LSTAT','AGE']].iloc[itest]))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 112, 
      "cell_type": "code", 
      "source": [
        "dfboston2=dfboston.copy()\n", 
        "dfboston2['LSTAT:AGE']=dfboston2['LSTAT']*dfboston2['AGE']\n", 
        "lr2 = LinearRegression()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 113, 
      "cell_type": "code", 
      "source": [
        "lr2.fit(dfboston2[['LSTAT','AGE','LSTAT:AGE']].iloc[itrain], boston.target[itrain])\n", 
        "mean_squared_error(boston.target[itest], lr2.predict(dfboston2[['LSTAT','AGE', 'LSTAT:AGE']].iloc[itest]))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "YOUR TURN HERE\n", 
        ">Just like `cv_optimize_ridge`, write a function for the Lasso `def cv_optimize_lasso(X, y, n_folds=5):` which uses `L1` regularization in linear regression. It can use the `cv_optimize` function from above"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 114, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 115, 
      "cell_type": "code", 
      "source": [
        "xtrain = dfboston.iloc[itrain]\n", 
        "xtest = dfboston.iloc[itest]\n", 
        "ytrain = boston.target[itrain]\n", 
        "ytest = boston.target[itest]\n", 
        "clf_boston = cv_optimize_lasso(xtrain, ytrain)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 121, 
      "cell_type": "code", 
      "source": [
        "clf_boston = clf_boston.best_estimator_\n", 
        "clf_boston.fit(xtrain, ytrain)\n", 
        "predicted = clf_boston.predict(xtest)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 122, 
      "cell_type": "code", 
      "source": [
        "mean_squared_error(ytest, predicted)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 123, 
      "cell_type": "code", 
      "source": [
        "plt.scatter(ytest, predicted)\n", 
        "plt.plot([0, 50], [0, 50], '--k')\n", 
        "plt.axis('tight')\n", 
        "plt.xlabel('True price ($1000s)')\n", 
        "plt.ylabel('Predicted price ($1000s)')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 140, 
      "cell_type": "code", 
      "source": [
        "def nonzero_lasso(clf, lcols):\n", 
        "    featuremask=(clf.coef_ !=0.0)\n", 
        "    return pd.DataFrame(dict(feature=lcols, coef=clf.coef_, abscoef=np.abs(clf.coef_)))[featuremask].sort('abscoef', ascending=False)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 141, 
      "cell_type": "code", 
      "source": [
        "list(dfboston.columns)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 142, 
      "cell_type": "code", 
      "source": [
        "clf_boston.coef_"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 143, 
      "cell_type": "code", 
      "source": [
        "lasso_importances=nonzero_lasso(clf_boston, list(dfboston.columns))\n", 
        "lasso_importances.set_index(\"feature\", inplace=True)\n", 
        "lasso_importances.head(10)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ]
}